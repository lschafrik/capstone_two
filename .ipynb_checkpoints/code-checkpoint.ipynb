{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from math import sqrt\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rent the Runway Pt. II\n",
    "\n",
    "\n",
    "## Prep the Data\n",
    "***\n",
    "- Load in data from aws s3\n",
    "- Sample data accordingly to prevent imbalance\n",
    "- Pull out review_text to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "df = pd.read_csv('s3://lschaf/capstone_two/rtr_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='Unnamed: 0', inplace=True)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit</th>\n",
       "      <th>user_id</th>\n",
       "      <th>bust_size</th>\n",
       "      <th>item_id</th>\n",
       "      <th>weight_lbs</th>\n",
       "      <th>rating</th>\n",
       "      <th>rented_for</th>\n",
       "      <th>review_text</th>\n",
       "      <th>body_type</th>\n",
       "      <th>review_summary</th>\n",
       "      <th>category</th>\n",
       "      <th>height_inches</th>\n",
       "      <th>size</th>\n",
       "      <th>age</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fit</td>\n",
       "      <td>420272</td>\n",
       "      <td>34d</td>\n",
       "      <td>2260466</td>\n",
       "      <td>137</td>\n",
       "      <td>10.0</td>\n",
       "      <td>vacation</td>\n",
       "      <td>An adorable romper! Belt and zipper were a lit...</td>\n",
       "      <td>hourglass</td>\n",
       "      <td>So many compliments!</td>\n",
       "      <td>romper</td>\n",
       "      <td>68.0</td>\n",
       "      <td>14</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2016-04-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fit</td>\n",
       "      <td>273551</td>\n",
       "      <td>34b</td>\n",
       "      <td>153475</td>\n",
       "      <td>132</td>\n",
       "      <td>10.0</td>\n",
       "      <td>other</td>\n",
       "      <td>I rented this dress for a photo shoot. The the...</td>\n",
       "      <td>straight &amp; narrow</td>\n",
       "      <td>I felt so glamourous!!!</td>\n",
       "      <td>gown</td>\n",
       "      <td>66.0</td>\n",
       "      <td>12</td>\n",
       "      <td>36.0</td>\n",
       "      <td>2013-06-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fit</td>\n",
       "      <td>909926</td>\n",
       "      <td>34c</td>\n",
       "      <td>126335</td>\n",
       "      <td>135</td>\n",
       "      <td>8.0</td>\n",
       "      <td>formal affair</td>\n",
       "      <td>I rented this for my company's black tie award...</td>\n",
       "      <td>pear</td>\n",
       "      <td>Dress arrived on time and in perfect condition.</td>\n",
       "      <td>dress</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014-02-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fit</td>\n",
       "      <td>151944</td>\n",
       "      <td>34b</td>\n",
       "      <td>616682</td>\n",
       "      <td>145</td>\n",
       "      <td>10.0</td>\n",
       "      <td>wedding</td>\n",
       "      <td>I have always been petite in my upper body and...</td>\n",
       "      <td>athletic</td>\n",
       "      <td>Was in love with this dress !!!</td>\n",
       "      <td>gown</td>\n",
       "      <td>69.0</td>\n",
       "      <td>12</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2016-09-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fit</td>\n",
       "      <td>734848</td>\n",
       "      <td>32b</td>\n",
       "      <td>364092</td>\n",
       "      <td>138</td>\n",
       "      <td>8.0</td>\n",
       "      <td>date</td>\n",
       "      <td>Didn't actually wear it. It fit perfectly. The...</td>\n",
       "      <td>athletic</td>\n",
       "      <td>Traditional with a touch a sass</td>\n",
       "      <td>dress</td>\n",
       "      <td>68.0</td>\n",
       "      <td>8</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2016-04-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit  user_id bust_size  item_id  weight_lbs  rating     rented_for  \\\n",
       "0  fit   420272       34d  2260466         137    10.0       vacation   \n",
       "1  fit   273551       34b   153475         132    10.0          other   \n",
       "2  fit   909926       34c   126335         135     8.0  formal affair   \n",
       "3  fit   151944       34b   616682         145    10.0        wedding   \n",
       "4  fit   734848       32b   364092         138     8.0           date   \n",
       "\n",
       "                                         review_text          body_type  \\\n",
       "0  An adorable romper! Belt and zipper were a lit...          hourglass   \n",
       "1  I rented this dress for a photo shoot. The the...  straight & narrow   \n",
       "2  I rented this for my company's black tie award...               pear   \n",
       "3  I have always been petite in my upper body and...           athletic   \n",
       "4  Didn't actually wear it. It fit perfectly. The...           athletic   \n",
       "\n",
       "                                     review_summary category  height_inches  \\\n",
       "0                              So many compliments!   romper           68.0   \n",
       "1                           I felt so glamourous!!!     gown           66.0   \n",
       "2  Dress arrived on time and in perfect condition.     dress           65.0   \n",
       "3                   Was in love with this dress !!!     gown           69.0   \n",
       "4                   Traditional with a touch a sass    dress           68.0   \n",
       "\n",
       "   size   age review_date  \n",
       "0    14  28.0  2016-04-20  \n",
       "1    12  36.0  2013-06-18  \n",
       "2     8  34.0  2014-02-12  \n",
       "3    12  27.0  2016-09-26  \n",
       "4     8  45.0  2016-04-30  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({10.0: 94196, 8.0: 40671, 4.0: 2183, 6.0: 8309, 2.0: 778})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y) # take 700 reviews from each rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a function for sampling, because the dataset is so large it's crashing my computer\n",
    "\n",
    "def sample_data(data, col, param, sample_size=100):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : Dataframe \n",
    "           Must have a column of the same name as the specified dataframe column.\n",
    "    col : (str) Dataframe column\n",
    "           Name of dataframe column to pull samples from.\n",
    "    param : List of unique features found in specified dataframe column \n",
    "           List of unique feature or features within specified column\n",
    "           to sample from.\n",
    "    sample_size : (int) \n",
    "           Size of random sampling to take from each parameter.\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas DataFrame of the randomly sampled data\n",
    "    \"\"\"\n",
    "    result_dict = {}\n",
    "    for par in param: \n",
    "        temp = data[data[col] == par]\n",
    "        if sample_size <= len(temp):\n",
    "            samp = temp.sample(n=sample_size)\n",
    "        else:\n",
    "            samp = temp.sample(n=len(temp))\n",
    "        result_dict[par] = samp\n",
    "    return pd.concat(result_dict.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = sample_data(df, 'rating', [2, 4, 6, 8, 10], 700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['text-summary'] = df_sample['review_text']+' '+df_sample['review_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the dress was way too short to wear and sheer. I didn't end up wearing it way too short\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample['text-summary'][23] # combine them because sometimes the summary is longer than the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function that takes the len of each text-summary minus stopwords/punc and drops all w len < num\n",
    "\n",
    "def remove_shortones(data, col, limit):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : Dataframe \n",
    "           Must have a column labelled 'text-summary'.\n",
    "    col : (str) Dataframe column\n",
    "           Name of dataframe column to filter and remove data from.\n",
    "    limit : (int) \n",
    "           Minimum number of words required within a row to remain within data; \n",
    "           if below the limit, the row is removed from the dataframe.       \n",
    "    Returns\n",
    "    -------\n",
    "    data : Dataframe\n",
    "        Returns data without the rows whose word count doesn't meet\n",
    "        or exceed the specified limit\n",
    "    \"\"\"\n",
    "    stopwords_ = set(stopwords.words('english'))\n",
    "    punctuation_ = set(string.punctuation)\n",
    "    temp = []\n",
    "    for idx, doc in enumerate(data[col]):\n",
    "        splt = doc.split()\n",
    "        flter = ([w for w in splt if not w in stopwords_ and not w in punctuation_])\n",
    "        if len(flter) < limit:\n",
    "            temp.append(idx)    \n",
    "    return data.drop(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = remove_shortones(df_sample, 'text-summary', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "strict_df = remove_shortones(df_sample, 'text-summary', 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3408, 16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strict_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum 5 words\n",
    "\n",
    "X = new_df.drop(columns='rating')\n",
    "y = new_df['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum 8 words\n",
    "\n",
    "X_s = strict_df.drop(columns='rating')\n",
    "y_s = strict_df['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function here to grab text only in order to run NLP stuff\n",
    "\n",
    "def get_review(data):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : Dataframe \n",
    "           Must have a column labelled 'text-summary'.\n",
    "          \n",
    "    Returns\n",
    "    -------\n",
    "    review_text : list of lists\n",
    "                  The review_text for each row within the dataframe.\n",
    "    review_summary : list of lists\n",
    "                  The review_summary for each row within the dataframe.\n",
    "    \"\"\"\n",
    "    review_text = []\n",
    "    review_summary = []\n",
    "\n",
    "    for review in data['text-summary']:\n",
    "        review_text.append(review)\n",
    "        \n",
    "    return review_text, review_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text, review_summary = get_review(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_review_text, s_review_summary = get_review(X_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep the Pipeline\n",
    "***\n",
    "- Create base pipeline with CountVectorizer/TfidfTransformer\n",
    "- Run review_text through base pipeline\n",
    "- Split the data\n",
    "    - *Splitting data after pipeline ensures there are no errors during modelling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pipe = Pipeline([('vect', CountVectorizer(strip_accents='ascii',\n",
    "                                               stop_words='english')),\n",
    "                         ('tfidf', TfidfTransformer(sublinear_tf=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3483, 5573)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectrev_gb = base_pipe.fit_transform(review_text)\n",
    "Xrev_gb = vectrev_gb.toarray()\n",
    "Xrev_gb.shape # Limit minimum of 5 words (3479, 5815)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3408, 5568)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_vectrev_gb = base_pipe.fit_transform(s_review_text)\n",
    "s_Xrev_gb = s_vectrev_gb.toarray()\n",
    "s_Xrev_gb.shape # Limit minimum of 8 words (3416, 5761)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split ya data - test_size 0.5 because feature counts need to match or models will error\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xrev_gb, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(s_Xrev_gb, y_s, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Test the Models\n",
    "***\n",
    "\n",
    "### *Can I predict what a user's rating might be based on review_text/summary?*\n",
    "- Use regression models\n",
    "    - *GradientBoostingRegressor*\n",
    "    - *RandomForestRegressor*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *GradientBoosting*\n",
    "- *This one works best*\n",
    "\n",
    "__*Winner Winner, Chicken Dinner*: Sampling 700, limiting text-summary with a minimum of 8 words__\n",
    "- `GradientBoostingRegressor(n_estimators=700, max_depth=4)`\n",
    "    - Train: 0.9576454189981304\n",
    "    - Test: 0.5404655058068318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 700 samples - works best\n",
    "    # Larger and smaller samples were attempted and abandoned due to their inability to create better scores\n",
    "    \n",
    "# gb_model = GradientBoostingRegressor(n_estimators=200, max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gb_fit = gb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gb_model.score(X_train, y_train))\n",
    "# print(gb_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `n_estimators=200, max_depth=5`\n",
    "    - Train: 0.8878962389242915\n",
    "    - Test: 0.5176185468139696\n",
    "- `n_estimators=600, max_depth=5`\n",
    "    - Train: 0.9737958004932334\n",
    "    - Test: 0.5032403223357067\n",
    "- `n_estimators=200, max_depth=5, min_samples_split=6`\n",
    "    - Train: 0.882621481308227\n",
    "    - Test: 0.5158991828873656"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_gb_model = GradientBoostingRegressor(n_estimators=700, max_depth=4)\n",
    "s_gb_fit = s_gb_model.fit(Xs_train, ys_train)\n",
    "print(s_gb_model.score(Xs_train, ys_train))\n",
    "print(s_gb_model.score(Xs_test, ys_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `n_estimators=200, max_depth=5`\n",
    "    - Train: 0.8773835548272503\n",
    "    - Test: 0.5250728671698321\n",
    "- `n_estimators=600, max_depth=5`\n",
    "    - Train: 0.9719742993420543\n",
    "    - Test: 0.5327350039174654\n",
    "- `n_estimators=600, max_depth=3`\n",
    "    - Train: 0.8990363662686204\n",
    "    - Test: 0.5358862503972373\n",
    "    \n",
    "    \n",
    "__*Winner Winner, Chicken Dinner*: Sampling 700, limiting text-summary with a minimum of 8 words__\n",
    "- `GradientBoostingRegressor(n_estimators=700, max_depth=4)`\n",
    "    - Train: 0.9576454189981304\n",
    "    - Test: 0.5404655058068318"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *RandomForest*\n",
    "- *Yo this one is wack, no way*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfc_model = RandomForestRegressor(n_jobs=-1, n_estimators=300, max_depth=5, oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfc_fit = rfc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rfc_model.score(X_train, y_train))\n",
    "# print(rfc_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Base\n",
    "    - Train: 0.9252327611282029\n",
    "    - Test: 0.4825992672489555\n",
    "- `n_jobs=-1, n_estimators=300, max_depth=5, oob_score=True`\n",
    "    - Train: 0.4508526404607375\n",
    "    - Test: 0.37753824805077096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_rfc_model = RandomForestRegressor(n_jobs=-1, n_estimators=300, max_depth=5, oob_score=True)\n",
    "# s_rfc_fit = s_rfc_model.fit(Xs_train, ys_train)\n",
    "# print(s_rfc_model.score(Xs_train, ys_train))\n",
    "# print(s_rfc_model.score(Xs_test, ys_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `n_jobs=-1, n_estimators=300, max_depth=5, oob_score=True`\n",
    "    - Train: 0.43924238866924414\n",
    "    - Test: 0.3687477848563787"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "***\n",
    "### *After this week's experiences, what suggestions would you provide for improvements within the company?*\n",
    "- The biggest issue is how __the review box is split in two__ - a title (summary) and the review itself (text).  This seems to create confusion, as there are a number of users who put their entire review into the summary box, and put one word into the text box.  I would suggest combining them into one box in which they can write their review, or better yet (to preserve the aesthetic of the review field, which I do love) __*limiting the word count for the summary box*__ so that users understand what the summary box is meant for.\n",
    "\n",
    "### *What would you do or change, with more time?*\n",
    "- I would love to see how well I could predict other features with the text-summary column\n",
    "- I would love to see the impact of the other columns on their importance in what a user's rating might be; does a user's weight affect their rating?  A user's age or height?\n",
    "- I would continue trying to improve my current model\n",
    "\n",
    "### *What troubles did you run into during this capstone?*\n",
    "- The biggest trouble was my computer's inability to handle the data and models.  It crashed multiple times, which led to the necessity to sample the data and work with a much smaller dataset.  Going forward, I'd probably try to use AWS or even the Google notebook in an attempt to ease the burden on my poor computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visuals\n",
    "***\n",
    "- Screenshots of site for reference *(done)*\n",
    "- Word cloud for best and worst ratings *(done)*\n",
    "- Graphs of errors to visualize model's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6)) # base dataset\n",
    "\n",
    "df['rating'].hist(color='indigo')\n",
    "\n",
    "plt.title('Count of Ratings in Base Dataset', fontsize='15')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6)) # after sampling to avoid imbalance and to save my computer\n",
    "\n",
    "df_sample['rating'].hist(color='teal')\n",
    "\n",
    "plt.title('Count of Ratings in Balanced Dataset', fontsize='15')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6)) # After removing rows where review < 8 words\n",
    "\n",
    "strict_df['rating'].hist(color='steelblue')\n",
    "\n",
    "plt.title('Balanced Dataset Rating Count after Limiting Based on Word Count', fontsize='15')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bar graph showing which ratings the model's having trouble with...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(data, params):\n",
    "    result_dict = {}\n",
    "    for param in params:\n",
    "        for lst in data:\n",
    "            if f\"{param}_True\" not in result_dict:\n",
    "                result_dict[f\"{param}_True\"] = 0\n",
    "            if f\"{param}_False\" not in result_dict:\n",
    "                result_dict[f\"{param}_False\"] = 0\n",
    "                \n",
    "            if lst[1] == param and lst[0] == True:\n",
    "                result_dict[f\"{param}_True\"] += 1\n",
    "            elif lst[1] == param and lst[0] == False:\n",
    "                result_dict[f\"{param}_False\"] += 1\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_pred = s_gb_model.predict(Xs_train)\n",
    "test_final_pred = s_gb_model.predict(Xs_test)\n",
    "\n",
    "ys_train_array = np.array(ys_train)\n",
    "ys_test_array = np.array(ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_5 = []\n",
    "train_5 = []\n",
    "\n",
    "for idx, val in enumerate(test_final_pred):\n",
    "    test_5.append([val < ys_test_array[idx]+.5 and val > ys_test_array[idx]-.5, ys_test_array[idx]])\n",
    "\n",
    "for idx, val in enumerate(train_final_pred):\n",
    "    train_5.append([val < ys_train_array[idx]+.5 and val > ys_train_array[idx]-.5, ys_train_array[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = get_counts(test_5, [2, 4, 6, 8, 10])\n",
    "\n",
    "train_dict = get_counts(train_5, [2, 4, 6, 8, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = [2, 4, 6, 8, 10]\n",
    "train_trues5 = [111,239,193,199,162]\n",
    "train_falses5 = [226,103,137,141,197]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.bar(r, train_trues5, color='cadetblue', edgecolor='white', label='Accurate')\n",
    "plt.bar(r, train_falses5, bottom=train_trues5, color='powderblue', edgecolor='white', label='Inaccurate')\n",
    "\n",
    "plt.title('Training Data Accurate v. Inaccurate Predictions, Based on a 0.5 Variation', fontsize='15')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trues5 = [63,181,163,164,98]\n",
    "test_falses5 = [265,155,197,185,237]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.bar(r, test_trues5, color='darkslategray', edgecolor='white', label='Accurate')\n",
    "plt.bar(r, test_falses5, bottom=test_trues5, color='darkseagreen', edgecolor='white', label='Inaccurate')\n",
    "\n",
    "plt.title('Testing Data Accurate v. Inaccurate Predictions, Based on a 0.5 Variation', fontsize='15')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
